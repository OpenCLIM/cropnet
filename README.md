CropNET Data Assimilation tool for UKCP18 wheat yield projections
-----------------------------------------------------------------

Overview
--------
This tool produces estimates of crop yield using a wheat crop model and MODIS2 LAI (leaf-area-index) satellite data.
The crop model produces estimates of potential yield, which are usually higher than the actual, measured yields.
The crop model is therefore combined with satellite derived measures of crop growth to produce more accurate values.
For each year of the UKCP18 driving (meteorlogical) data that we also have satellite data for:
- The satellite data is requested and downloaded if necessary
- The modelled GAI over the year is calculated for each ensemble member
- The ensemble variance is used to generate a baseline estimate of the uncertainty in the modelled GAI
- A variational data assimilation method is applied to the timeseries of the satellite LAI at each pixel downloaded
  and the model GAI for the corresponding grid point within which it resides. This produces an optimum, smoothed
  timeseries that is the best guess of the true GAI, given the errors associated with the model and the observed values.
  See below for more details on the method.
- The optimal GAI timeseries is fed back in to the crop model and used to calculate a new yield value

Once this is complete, the average correction factor vector is calculated over each year and ensemble member, for each
observation pixel used.
TO BE IMPLEMENTED:
Using the data assimilation for future years, using a simple linear regression approach, or machine learning to tailor the
assimilation to each field. 

Data Assimilation details
-------------------------
A method a bit like 4Dvar that attempts to minimize a cost function which consists of terms that grow in magnitude the
further the assimilated/guessed timeseries is away from a) the modelled GAI, b) the observations on the days that they
exist and c) a smoothed version of the timeseries itself, to find the optimum timeseries given these constraints.

If obs are not provided, some sudo-obs can be generated from one of the model ensemble members. The fake observations
are generated by using one of the model ensemble members, reducing it, adding some noise and deleting everything but
every 16th timestep.

The strength of each constraint is dependent on the errors (variances) associated with each timeseries. The smaller the
error, the more strongly the optimal timeseries is pulled towards the timeseries. These are user configurable.
By default, the error in the modelled timeseries is assumed to be the variance of the ensemble. There is a multiplier (default 1)
that can be changed to account for errors in the model not accounted for by errors in the meteorlogical driving data. 
The error in the observed timeseries/points is assumed to be 0.1 regardless of the magnitude of the observation. This
absolute error can be switched to a percentage error, whereby the error is assumed to be, e.g. 10% of the obs value.
However, in testing it was found that the absolute error produces much smoother results, particularly early on in the
timeseries when the observation values are small.
The error in the smoothed timeseries is a little more complex, but can be understood as the value by which the timeseries
should not change by more than for one timestep to the next. This is set to vary depending on the magnitude of each value in the
modelled GAI.
If the values at a part of the modelled GAI timeseries are small, then this error is smaller, and thus the smoothing constraint is
stronger for this part of the timeseries, and vice versa.
Other controls on the strength of the smoothing constraint exist. These are power and order.
Power controls the overall strength of the constraint throughout the timeseries, so the higher this is, the smoother
the optimal timeseries will be.
Order controls how it does the smoothing. An order of 1 will only consider 1st order differences, 2, 2nd order differences
etc. For example, order 2 will try to make the derivative of the optimal timeseries smooth, which can result in the
actual timeseries being very variable!
The defaults of power 10 and order 1 have been set through trial and error to provide the best results.
They are user configurable, but change them at your peril!!

Data to be provided by the user
-------------------------------
The meteorological data used to drive the model must be provided by the user in a specific format. The code has been designed
to work with UKCP18 data, but in theory any meteorological driving data can be used, provided it meets the format requirements.
Note though, that this has not been tested. Future versions of the code will remove these stringent restrictions.
 - The daily variables required are total precipitation in mm (pr), maximum 2m temperature in degC (tasmax),
minimum 2m temperature in degC (tasmin), average net surface longwave radiation in W/m^2 (rls), 
average net surface shortwave radiation in W/m^2 (rss), average 2m temperature in degC (tas).
 - The filenames must contain the variable names in brackets, which must also be the variable names within the netCDF files.
 - The end of each filename must also be written _nn_startdate_enddate.nc with startdate and enddate formatted yyyymmdd. 
nn is ensemble number, but can be any string. 
- The data must also be on an x,y grid where x and y are OSGB eastings and northings. The variable names of the x and y
variables must be projection_x_coordinate and projection_y_coordinate. 
- The data must be on a 360day calendar.
- Finally, the data must contain a variable named, literally, yyyymmdd, which contains the yyyymmdd strings for each timestep
in the datefile. 

If you wish to verify the yield produced by the assimilation, you will need to set the verify switch to 1 and 
supply yield data in the form of a shapefile containing the polygons of each field with coordinates in OSGB eastings 
and northings. We cannot supply this data due to confidentiality arrangements with the data suppliers. 


Running the code
----------------
The code is written in python, which calls the crop model written in R.
The code uses several python/R packages that are not installed by default on the UKCEH Linux systems, or JASMIN.
The easiest way to install these is to use anaconda - a package management system for python and R modules/packages.
Anaconda installs packages in your home directory, and can be managed separately to any other python/R installations
you have, i.e. it won't interfere with these, and can be enabled only when you want to run this programme.
See below for instructions on installing and using anaconda.

The code can either be run from a python script or a jupyter notebook. If you have not heard of the latter, then it
is probably easier to use the former.
To run from the script:
- Open wrapper.py in a text editor and change any of the user-editable values at the top that you need to
- Load the anaconda environment as below
- Load ipython (or just python, but ipython looks nicer!) from the directory with the script in
- Then type 'run wrapper.py'

To run from the notebook:
- Load the anaconda environment as below
- Run 'jupyter notebook assimilation.ipynb' in the directory containing this ipynb file. This will open up a web browser
  with the notebook loaded.
- Run the cell with all the import statements in
- Change the user-editable variables in the next code cell as necessary and run it
- Run the main code in the cell below

The code is set up to run for the x,y coordinates specified by the obscoords variable, for a single year or multiple years,
for each UKCP18 ensemble member. The first time it is run it will download the required MODIS LAI data for assimilation.
After this, if the xy coords and years have not been changed, the obs switch can be set to 2, so that it uses what is already
downloaded. 
The first time it is run it will also format the netcdf UKCP18 data into an R datacube format. Future times it is run,
the genRdata switch can be set to 0. In future versions of the code, it will be changed to use the netCDF data directly.

The final output of the code will be the original, modelled yield and the updated, assimilated yield in xarray datasets,
which are similar in structure to netcdf files. The data will also be saved on disk as netcdf files. The datasets will
have one variable per obs pixel, with the name of the variable the x,y coordinates in OSGB eastings/northings. Each
variable will have ensemble member and year dimensions.

For information on the other user configurable variables, outputs and internal functions, see the comments written in the
notebook and scripts. 


Anaconda installation instructions
----------------------------------
To install anaconda and my python/R packages:

1. Download anaconda from https://www.anaconda.com
You want a python>=3.7, 64bit (x86) installer for linux.
As of 08/06/2020 this can be obtained by running: wget https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh
But note that the link to the most up to date version will likely change, so don't rely on this.

2. Install anaconda by running: bash the-script-you-just-downloaded.sh
By default it will install in your home directory, requiring no admin/root permissions.
At the end of the installation it will ask you whether you want to run conda-init.
Say no to this.

3. Add the following line to your .bashrc file in your home directory. If you don't have one, create one.
alias loadconda='export PATH="/path/to/homedir/anaconda3/bin:$PATH"'
Obtain the full path to your home directory by running pwd in your home directory.

4. Start a new bash shell by running bash, then load anaconda by running loadconda (the command we just created above).

5. Setup a 'DAenv' environment that contains all the modules you need to run my scripts,
using the conda_env_file_DAenv.txt. (You can change the name if you want). 
In your home directory run 'bash' if you are not already using the bash shell (by default UKCEH uses the csh shell)
Then conda create --name DAenv --file conda_env_file_DAenv.txt (after copying the txt file to your home dir).

6. Run 'source activate DAenv' to load this environment, and you should be good to go.

Now, instead of sourcing the python and R modules from the default setup, or your own, it will source them
from ~/anaconda3/envs/DAenv/.
To get back to your own python/R setup, simply open a new terminal/shell.

Anytime you want to run these scripts, or use my python setup, run:
bash
loadconda
source activate DAenv
